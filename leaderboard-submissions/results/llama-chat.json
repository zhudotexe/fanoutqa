{
  "_submission_hash": "",
  "_results_hash": "",
  "metadata": {
    "name": "LLaMA 2 70B",
    "authors": "Meta",
    "url": "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/",
    "citation": "Touvron et al., 2023",
    "type": "FOUNDATION",
    "context": 4096
  },
  "closedbook": {
    "acc": {
      "loose": 0.4403229470249054,
      "strict": 0.058011049723756904
    },
    "rouge": {
      "rouge1": {
        "precision": 0.21079994013879805,
        "recall": 0.5309704790876308,
        "fscore": 0.2847323680812482
      },
      "rouge2": {
        "precision": 0.11206570754887477,
        "recall": 0.26618658655979804,
        "fscore": 0.14873440544450633
      },
      "rougeL": {
        "precision": 0.17624114336611502,
        "recall": 0.44614569152308986,
        "fscore": 0.23787236888912539
      }
    },
    "bleurt": 0.44125290332331185,
    "gpt": 0.12016574585635359
  },
  "openbook": {
    "acc": {
      "loose": 0.38953698927856806,
      "strict": 0.06353591160220995
    },
    "rouge": {
      "rouge1": {
        "precision": 0.09966641622516868,
        "recall": 0.47799987641310493,
        "fscore": 0.15662035695680138
      },
      "rouge2": {
        "precision": 0.04823947632622272,
        "recall": 0.21575914217888367,
        "fscore": 0.07543987012466698
      },
      "rougeL": {
        "precision": 0.08324811278648249,
        "recall": 0.40852712042925454,
        "fscore": 0.13141945954530423
      }
    },
    "bleurt": 0.4429991969951938,
    "gpt": 0.10773480662983426
  },
  "evidenceprovided": {
    "acc": {
      "loose": 0.5143679183027113,
      "strict": 0.07734806629834254
    },
    "rouge": {
      "rouge1": {
        "precision": 0.3407245588154435,
        "recall": 0.5902473705256917,
        "fscore": 0.3762628122180759
      },
      "rouge2": {
        "precision": 0.1832699274716267,
        "recall": 0.31368060953904164,
        "fscore": 0.20593914103223815
      },
      "rougeL": {
        "precision": 0.27518702361418795,
        "recall": 0.4795536695039635,
        "fscore": 0.30410024296144206
      }
    },
    "bleurt": 0.47221096980678773,
    "gpt": 0.16160220994475138
  }
}